# README
## References
1. MUELLER, Jonas et THYAGARAJAN, Aditya. Siamese recurrent architectures for learning sentence similarity. In : Proceedings of the AAAI conference on artificial intelligence. 2016.
## 1. Introduction
Dans cet exercice, r√©seau siamois dans une t√¢che de "text-similarity" sur la dataset Quora est explor√©. L'architecture de base propos√©e est un siamois √† base de LSTM et la distance Manhatten inspir√© de l'√©tude men√©e par [1]. Dans cette √©tude, le mod√®le siamois de base est entrain√© sur la tache de similarit√© entre des pairs de questions du dataset Quora. Ensuite, l'impact de l'ajout du m√©chanisme d'attention (avant et apr√®s la couche LSTM) est √©tudi√© sur les performances du r√©seau √† trouver des similarit√© des questions. En fin, les couches d'embedding sont remplac√© par des embedding du mod√®le Bert, et une √©tude sur l'impact de l'ajout de cet LLM sur les performances est men√©e.


## 2. Siamese LSTM
 
Le r√©seau siamois de base est constitu√© de LSTM (Long Short-Term Memory) pour √©valuer la similarit√© s√©mantique entre phrases. Ce mod√®le exploite des vecteurs d'embeddings de mots enrichis d'informations synonymiques pour capturer le sens profond des phrases. En calculant une m√©trique de Manhattan, le mod√®le oblige les repr√©sentations des phrases √† former un espace  structur√©, donnant des relations s√©mantiques complexes. 

### Architecture du Siamese LSTM

<p align="center">
  <img src="images/siamese_lstm.png" alt="Siamese LSTM Architecture" width="500">
</p>
<p align="center"><em>Figure 1: Architecture du Siamese LSTM</em></p>



## 3. Attention + Siamese LSTM
Un mechanisme de self attention est ajout√© apr√®s(ou avant) la couche LSTM. Les courbes suivantes montre une comparaison entre les performances en accuracy du mo√®le avec couche d'attention (avant et apr√®s LSTM) et sans attention.
<p align="center">
  <img src="images/history-beforeattenvslstm-graph.png" alt="Siamese LSTM Architecture" width="500">
</p>
<p align="center"><em>Figure 2: Accuracy pour les mod√®les avec attention avant LSTM et sans attention</em></p>

<p align="center">
  <img src="images/history-attenvslstm-graph.png" alt="Siamese LSTM Architecture" width="500">
</p>
<p align="center"><em>Figure 3: Accuracy pour les mod√®les avec attention avant LSTM et sans attention</em></p>


La couche d'attention plac√©e apr√®s la couche LSTM, augmente les performances en accuracy d'entrainement de mani√®re significative. Le mod√®le apprend plus vite les similarit√©s entre pair. L'accuracy de validation est un meilleur d'un degr√©e moins significatif que dans l'entrainement. 
Pour expliquer le r√©sultat obtenu, il est constat√© que la LSTM a d√©j√† captur√© l'information de toute la s√©quence, y compris les d√©pendances √† long terme. L'attention peut donc se baser sur des repr√©sentations plus riches pour choisir les parties importantes.
La couche d'attention plac√©e avant la couche LSTM n'am√©liore pas les performances du mod√®le, la LSTM n'a pas encore vu la s√©quence compl√®te, donc l'attention est bas√©e uniquement sur les informations locales, ce qui provoque une perte du contexte globale.

Les poids d'attention peuvent √™tre visualis√© dans les figures suivantes. La valeur √† la position (ùëñ,ùëó) dans la matrice d'attention repr√©sente l'importance que la position ùëñ de la s√©quence accorde √† la position ùëó de la s√©quence. 
<p align="center">
  <img src="images/hml.png" alt="Siamese LSTM Architecture" width="500">
</p>
<p align="center"><em>Figure 4: Poids d'attention d'une sequence de test (pair droite)</em></p>

<p align="center">
  <img src="images/hmr.png" alt="Siamese LSTM Architecture" width="500">
</p>
<p align="center"><em>Figure 4: Poids d'attention d'une sequence de test (pair gauche)</em></p>

Cela peut indiquer que les √©l√©ments vers la fin de la s√©quence contiennent des informations cruciales pour le mod√®le, et ces √©l√©ments sont importants pour les positions plus t√¥t dans la s√©quence. Ceci peut √™tre expliqu√© par la nature des donn√©es trait√© (questions), la fin de la phrase contient des √©l√©ments importants pour la compr√©hension s√©mantique.

## 4. Bert

## Utilisation

Pour utiliser ce projet, suivez les √©tapes ci-dessous :
1. Clonez le d√©p√¥t : `git clone https://github.com/AkramBenamar/test_efrei.git`
2. Entrainer et √©valuer le mod√®le siamois lstm : `python main.py --model siamese_lstm --data_directory path_to_data --max_seq_length 20 --sample_size 10000 --n_epoch 50 --batch_size 2048`
3. avec attention : `python main.py --model attention_siamese_lstm --data_directory pat_to_data --max_seq_length 20 --sample_size 10000 --n_epoch 50 --batch_size 2048`

