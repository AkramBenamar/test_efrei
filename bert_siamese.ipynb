{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#utils"
      ],
      "metadata": {
        "id": "2oTRYF6rrdxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh-Q0AwTuv-_",
        "outputId": "cf0e1046-503b-4234-8788-7a23172e1cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t37xAgZzyDGf",
        "outputId": "36563662-553f-4c8d-b781-47320f81e3ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZF4g_s7rKvX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "import gensim\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import itertools\n",
        "\n",
        "\n",
        "def text_to_word_list(text):\n",
        "    # Pre process and convert texts to a list of words\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    text = text.split()\n",
        "\n",
        "    return text\n",
        "\n",
        "def make_w2v_embeddings(df, embedding_dim=300, empty_w2v=False):\n",
        "    vocabs = {}\n",
        "    vocabs_cnt = 0\n",
        "\n",
        "    vocabs_not_w2v = {}\n",
        "    vocabs_not_w2v_cnt = 0\n",
        "\n",
        "    # Stopwords\n",
        "    stops = set(stopwords.words('english'))\n",
        "\n",
        "    # Load word2vec\n",
        "    print(\"Loading word2vec model(it may takes 2-3 mins) ...\")\n",
        "\n",
        "    if empty_w2v:\n",
        "        word2vec = EmptyWord2Vec\n",
        "    else:\n",
        "        # word2vec = KeyedVectors.load_word2vec_format(\"./data/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
        "        word2vec = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
        "\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Print the number of embedded sentences.\n",
        "        if index != 0 and index % 1000 == 0:\n",
        "            print(\"{:,} sentences embedded.\".format(index), flush=True)\n",
        "\n",
        "        # Iterate through the text of both questions of the row\n",
        "        for question in ['question1', 'question2']:\n",
        "\n",
        "            q2n = []  # q2n -> question numbers representation\n",
        "            for word in text_to_word_list(row[question]):\n",
        "                # Check for unwanted words\n",
        "                if word in stops:\n",
        "                    continue\n",
        "\n",
        "                # If a word is missing from word2vec model.\n",
        "                if word not in word2vec.key_to_index:\n",
        "                    if word not in vocabs_not_w2v:\n",
        "                        vocabs_not_w2v_cnt += 1\n",
        "                        vocabs_not_w2v[word] = 1\n",
        "\n",
        "                # If you have never seen a word, append it to vocab dictionary.\n",
        "                if word not in vocabs:\n",
        "                    vocabs_cnt += 1\n",
        "                    vocabs[word] = vocabs_cnt\n",
        "                    q2n.append(vocabs_cnt)\n",
        "                else:\n",
        "                    q2n.append(vocabs[word])\n",
        "\n",
        "            # Append question as number representation\n",
        "            df.at[index, question + '_n'] = q2n\n",
        "\n",
        "    embeddings = 1 * np.random.randn(len(vocabs) + 1, embedding_dim)  # This will be the embedding matrix\n",
        "    embeddings[0] = 0  # So that the padding will be ignored\n",
        "\n",
        "    # Build the embedding matrix\n",
        "    for word, index in vocabs.items():\n",
        "        if word in word2vec.key_to_index:\n",
        "            embeddings[index] = word2vec.get_vector(word)\n",
        "    del word2vec\n",
        "\n",
        "    return df, embeddings\n",
        "\n",
        "def make_w2v_embeddings2(df, embedding_dim=300, empty_w2v=False):\n",
        "    vocabs = {}\n",
        "    vocabs_cnt = 0\n",
        "\n",
        "    vocabs_not_w2v = {}\n",
        "    vocabs_not_w2v_cnt = 0\n",
        "\n",
        "    # Stopwords\n",
        "    stops = set(stopwords.words('english'))\n",
        "\n",
        "    # Load word2vec\n",
        "    print(\"Loading word2vec model(it may takes 2-3 mins) ...\")\n",
        "\n",
        "    if empty_w2v:\n",
        "        word2vec = EmptyWord2Vec\n",
        "    else:\n",
        "        word2vec = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/compairedataset/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
        "        # word2vec = gensim.models.word2vec.Word2Vec.load(\"./data/Quora-Question-Pairs.w2v\").wv\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Print the number of embedded sentences.\n",
        "        # if index != 0 and index % 1000 == 0:\n",
        "        #     print(\"{:,} sentences embedded.\".format(index), flush=True)\n",
        "\n",
        "        # Iterate through the text of both questions of the row\n",
        "        for question in ['question1', 'question2']:\n",
        "\n",
        "            q2n = []  # q2n -> question numbers representation\n",
        "            for word in text_to_word_list(row[question]):\n",
        "                # Check for unwanted words\n",
        "                if word in stops:\n",
        "                    continue\n",
        "\n",
        "                # If a word is missing from word2vec model.\n",
        "                if word not in word2vec.vocab:\n",
        "                    if word not in vocabs_not_w2v:\n",
        "                        vocabs_not_w2v_cnt += 1\n",
        "                        vocabs_not_w2v[word] = 1\n",
        "\n",
        "                # If you have never seen a word, append it to vocab dictionary.\n",
        "                if word not in vocabs:\n",
        "                    vocabs_cnt += 1\n",
        "                    vocabs[word] = vocabs_cnt\n",
        "                    q2n.append(vocabs_cnt)\n",
        "                else:\n",
        "                    q2n.append(vocabs[word])\n",
        "\n",
        "            # Append question as number representation\n",
        "            df.at[index, question + '_n'] = q2n\n",
        "\n",
        "    embeddings = 1 * np.random.randn(len(vocabs) + 1, embedding_dim)  # This will be the embedding matrix\n",
        "    embeddings[0] = 0  # So that the padding will be ignored\n",
        "\n",
        "    # Build the embedding matrix\n",
        "    for word, index in vocabs.items():\n",
        "        if word in word2vec.vocab:\n",
        "            embeddings[index] = word2vec.word_vec(word)\n",
        "    del word2vec\n",
        "\n",
        "    return df, embeddings\n",
        "\n",
        "\n",
        "def split_and_zero_padding(df, max_seq_length):\n",
        "    # Split to dicts\n",
        "    X = {'left': df['question1_n'], 'right': df['question2_n']}\n",
        "\n",
        "    # Zero padding\n",
        "    for dataset, side in itertools.product([X], ['left', 'right']):\n",
        "        dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "class EmptyWord2Vec:\n",
        "    \"\"\"\n",
        "    Just for test use.\n",
        "    \"\"\"\n",
        "    vocab = {}\n",
        "    word_vec = {}\n",
        "from tensorflow.keras.layers import Layer\n",
        "import tensorflow as tf\n",
        "\n",
        "class ManDist(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ManDist, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        left, right = inputs\n",
        "        return tf.exp(-tf.reduce_sum(tf.abs(left - right), axis=1, keepdims=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.models import Model, Sequential\n",
        "from keras.layers import Input, Embedding, LSTM, GRU, Conv1D, Conv2D, GlobalMaxPool1D, Dense, Dropout\n",
        "\n",
        "\n",
        "\n",
        "TRAIN_CSV = '/content/drive/MyDrive/compairedataset/questions.csv'\n",
        "\n",
        "# Load training set\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "train_df = train_df.sample(n=10000, random_state=42)\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "for q in ['question1', 'question2']:\n",
        "    train_df[q + '_n'] = train_df[q]\n",
        "\n",
        "# Make word2vec embeddings\n",
        "embedding_dim = 300\n",
        "max_seq_length = 20\n",
        "use_w2v = True\n",
        "\n",
        "# train_df, embeddings = make_w2v_embeddings(train_df, embedding_dim=embedding_dim, empty_w2v=not use_w2v)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "# Split to train validation\n",
        "validation_size = int(len(train_df) * 0.1)\n",
        "training_size = len(train_df) - validation_size\n",
        "\n",
        "X = train_df[['question1_n', 'question2_n']]\n",
        "Y = train_df['is_duplicate']\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYDa6V1KE1rq",
        "outputId": "675a7bca-b03f-44fb-b2f9-e70daf3e93b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras.models import Model, Sequential\n",
        "from keras.layers import Input, Embedding, LSTM, GRU, Conv1D, Conv2D, GlobalMaxPool1D, Dense, Dropout\n",
        "\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a5iPlruHud9",
        "outputId": "045074ec-74e8-4464-8bd1-678d5cdbe94d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "removes =[]\n",
        "# for i,xx in enumerate(X[\"question1_n\"]):\n",
        "for i,xx in enumerate(X[\"question1_n\"]):\n",
        "  q1, q2 = X[\"question1_n\"][i],X[\"question2_n\"][i]\n",
        "  if type(q1) != type(\"a\") or type(q2) != type(\"a\"):\n",
        "    removes.append(i)\n",
        "\n",
        "print(removes)\n",
        "\n",
        "  # if (type(xx[\"question1_n\"])!=type(\"a\")) or (type(xx[\"question2_n\"])!=type(\"a\")):\n",
        "  #   print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYAUZXS-x5uR",
        "outputId": "399f8759-49f9-4bf5-f996-7a0b108fe758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.drop(removes)\n",
        "Y = Y.drop(removes)"
      ],
      "metadata": {
        "id": "iLtxZCVSzRnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)"
      ],
      "metadata": {
        "id": "tXq5YOl_0QLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1_pre, X2_pre = X_train[\"question1_n\"], X_train[\"question2_n\"]\n",
        "#\n",
        "X1_pre_val, X2_pre_val = X_validation[\"question1_n\"], X_validation[\"question2_n\"]\n"
      ],
      "metadata": {
        "id": "o-oMQOMU0Wm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokinzed_data1_n = tokenizer(X1_pre.to_list(),  return_tensors=\"np\", max_length= 20, padding=\"max_length\", truncation=True)\n",
        "tokinzed_data2_n = tokenizer(X2_pre.to_list(),  return_tensors=\"np\", max_length= 20, padding=\"max_length\", truncation=True)"
      ],
      "metadata": {
        "id": "HZDTAPgvn9cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokinzed_data1_n_val = tokenizer(X1_pre_val.to_list(),  return_tensors=\"np\", max_length= 20, padding=\"max_length\", truncation=True)\n",
        "tokinzed_data2_n_val = tokenizer(X2_pre_val.to_list(),  return_tensors=\"np\", max_length= 20, padding=\"max_length\", truncation=True)"
      ],
      "metadata": {
        "id": "H0mv8vib1uK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_ids_data1 = tokinzed_data1_n[\"input_ids\"]\n",
        "inputs_ids_data2 = tokinzed_data2_n[\"input_ids\"]\n",
        "\n",
        "masks_data1 = tokinzed_data1_n[\"attention_mask\"]\n",
        "masks_data2 = tokinzed_data1_n[\"attention_mask\"]\n",
        "\n",
        "# val\n",
        "\n",
        "inputs_ids_data1_val = tokinzed_data1_n_val[\"input_ids\"]\n",
        "inputs_ids_data2_val = tokinzed_data2_n_val[\"input_ids\"]\n",
        "\n",
        "masks_data1_val = tokinzed_data1_n_val[\"attention_mask\"]\n",
        "masks_data2_val = tokinzed_data2_n_val[\"attention_mask\"]\n"
      ],
      "metadata": {
        "id": "EhDCamCQtMDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs_ids_data1.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiXKQH5oEAdc",
        "outputId": "431b3a33-58a3-47f4-81c9-123397832bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9000, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.save('/content/drive/MyDrive/compairedataset/inputs_ids_data1.npy',inputs_ids_data1)\n",
        "np.save('/content/drive/MyDrive/compairedataset/inputs_ids_data2.npy',inputs_ids_data2)\n",
        "np.save('/content/drive/MyDrive/compairedataset/masks_data1.npy',masks_data1)\n",
        "np.save('/content/drive/MyDrive/compairedataset/masks_data2.npy',masks_data2)\n",
        "\n",
        "np.save('/content/drive/MyDrive/compairedataset/inputs_ids_data1_val.npy',inputs_ids_data1_val)\n",
        "np.save('/content/drive/MyDrive/compairedataset/inputs_ids_data2_val.npy',inputs_ids_data2_val)\n",
        "np.save('/content/drive/MyDrive/compairedataset/masks_data1_val.npy',masks_data1_val)\n",
        "np.save('/content/drive/MyDrive/compairedataset/masks_data2_val.npy',masks_data2_val)"
      ],
      "metadata": {
        "id": "ZpxUMKoIBaQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_ids_data11=np.load('/content/drive/MyDrive/compairedataset/inputs_ids_data1.npy')\n",
        "inputs_ids_data11.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2nS47J_EILW",
        "outputId": "e5fa4139-18b0-4a93-b4a3-977a11fe076b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(363913, 286)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "inputs_ids_data1=np.load('/content/drive/MyDrive/compairedataset/inputs_ids_data1.npy')\n",
        "inputs_ids_data2=np.load('/content/drive/MyDrive/compairedataset/inputs_ids_data2.npy')\n",
        "masks_data1=np.load('/content/drive/MyDrive/compairedataset/masks_data1.npy')\n",
        "masks_data2=np.load('/content/drive/MyDrive/compairedataset/masks_data2.npy')\n",
        "\n",
        "inputs_ids_data1_val=np.load('/content/drive/MyDrive/compairedataset/inputs_ids_data1_val.npy')\n",
        "inputs_ids_data2_val=np.load('/content/drive/MyDrive/compairedataset/inputs_ids_data2_val.npy')\n",
        "masks_data1_val=np.load('/content/drive/MyDrive/compairedataset/masks_data1_val.npy')\n",
        "masks_data2_val=np.load('/content/drive/MyDrive/compairedataset/masks_data2_val.npy')"
      ],
      "metadata": {
        "id": "wkFCX-lGCLgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/gpu:0'):\n",
        "    inputs_ids_data1_tensor = tf.convert_to_tensor(inputs_ids_data1)\n",
        "    inputs_ids_data2_tensor = tf.convert_to_tensor(inputs_ids_data2)\n",
        "    masks_data1_tensor = tf.convert_to_tensor(masks_data1)\n",
        "    masks_data2_tensor = tf.convert_to_tensor(masks_data2)\n",
        "\n",
        "    inputs_ids_data1_val_tensor = tf.convert_to_tensor(inputs_ids_data1_val)\n",
        "    inputs_ids_data2_val_tensor = tf.convert_to_tensor(inputs_ids_data2_val)\n",
        "    masks_data1_val_tensor = tf.convert_to_tensor(masks_data1_val)\n",
        "    masks_data2_val_tensor = tf.convert_to_tensor(masks_data2_val)"
      ],
      "metadata": {
        "id": "1z6n3W3TF7mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del inputs_ids_data1\n",
        "del inputs_ids_data2\n",
        "del masks_data1\n",
        "del masks_data2\n",
        "\n",
        "del inputs_ids_data1_val\n",
        "del inputs_ids_data2_val\n",
        "del masks_data1_val\n",
        "del masks_data2_val"
      ],
      "metadata": {
        "id": "nm39L3-gJQgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm9dSY0AJqBu",
        "outputId": "6995013a-78b2-4fb3-9946-ca6c276c94df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "476"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs_ids_data1_val.shape)\n",
        "print(inputs_ids_data1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIe-BONF62Xr",
        "outputId": "e9e99a85-1d3a-49e0-85ed-0ab56e2c3ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40435, 286)\n",
            "(363913, 286)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_ids_data2.shape"
      ],
      "metadata": {
        "id": "vpGPGn7y5P96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#with bert"
      ],
      "metadata": {
        "id": "si1K1qwOKy49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertModel, BertTokenizer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Layer, LSTM\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BertLayer(Layer):\n",
        "    def __init__(self, bert_model, **kwargs):\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "        self.bert = bert_model\n",
        "\n",
        "    def call(self, inputs, masks):\n",
        "        return self.bert(inputs, masks)[0]\n",
        "\n",
        "\n",
        "class SelfAttention(Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super(SelfAttention, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W_q = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                   initializer='random_normal',\n",
        "                                   trainable=True)\n",
        "        self.W_k = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                   initializer='random_normal',\n",
        "                                   trainable=True)\n",
        "        self.W_v = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                   initializer='random_normal',\n",
        "                                   trainable=True)\n",
        "        super(SelfAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query = tf.tensordot(inputs, self.W_q, axes=1)\n",
        "        key = tf.tensordot(inputs, self.W_k, axes=1)\n",
        "        value = tf.tensordot(inputs, self.W_v, axes=1)\n",
        "        scores = tf.matmul(query, key, transpose_b=True)\n",
        "        scores /= tf.math.sqrt(tf.cast(tf.shape(key)[-1], tf.float32))\n",
        "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
        "        context_vector = tf.matmul(attention_weights, value)\n",
        "        return context_vector\n",
        "\n",
        "\n",
        "max_seq_length = 20\n",
        "batch_size = 256\n",
        "n_epoch = 10\n",
        "\n",
        "left_input = Input(shape=(max_seq_length,), dtype='int32', name='left_input')\n",
        "right_input = Input(shape=(max_seq_length,), dtype='int32', name='right_input')\n",
        "\n",
        "left_input_mask = Input(shape=(max_seq_length,), dtype='int32', name='left_input_masks')\n",
        "right_input_mask = Input(shape=(max_seq_length,), dtype='int32', name='right_input_masks')\n",
        "\n",
        "bert_layer = BertLayer(bert_model)#BertLayer(bert_model)\n",
        "bert_output_left = bert_layer(left_input, left_input_mask)\n",
        "bert_output_right =bert_layer(right_input, right_input_mask)\n",
        "\n",
        "\n",
        "attention_units = 64\n",
        "lstm_units = 50\n",
        "\n",
        "attention = SelfAttention(attention_units)\n",
        "\n",
        "attention_left = attention(bert_output_left)\n",
        "attention_right = attention(bert_output_right)\n",
        "\n",
        "LSTM_cell = LSTM(lstm_units, return_sequences= True)\n",
        "\n",
        "lstm_left = LSTM_cell(attention_left)\n",
        "lstm_right = LSTM_cell(attention_right)\n",
        "\n",
        "pooled_left = tf.keras.layers.GlobalAveragePooling1D()(lstm_left)\n",
        "pooled_right = tf.keras.layers.GlobalAveragePooling1D()(lstm_right)\n",
        "\n",
        "malstm_distance = ManDist()([pooled_left, pooled_right])\n",
        "\n",
        "\n",
        "model = Model(inputs=[left_input, right_input, left_input_mask, right_input_mask], outputs=[malstm_distance])\n",
        "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "training_start_time = time()\n",
        "atten_malstm_trained = model.fit([inputs_ids_data1, inputs_ids_data2, masks_data1, masks_data2 ], Y_train,\n",
        "                                 batch_size=batch_size, epochs=n_epoch,\n",
        "                                 validation_data=([inputs_ids_data1_val, inputs_ids_data2_val, masks_data1_val, masks_data2_val], Y_validation))\n",
        "# atten_malstm_trained = model.fit(\n",
        "#     [inputs_ids_data1_tensor, inputs_ids_data2_tensor, masks_data1_tensor, masks_data2_tensor],  # Use GPU tensors\n",
        "#     Y_train,\n",
        "#     batch_size=batch_size,\n",
        "#     epochs=n_epoch,\n",
        "#     validation_data=(\n",
        "#         [inputs_ids_data1_val_tensor, inputs_ids_data2_val_tensor, masks_data1_val_tensor, masks_data2_val_tensor],  # Use GPU tensors\n",
        "#         Y_validation\n",
        "#     )\n",
        "# )\n",
        "training_end_time = time()\n",
        "\n",
        "print(\"Training time finished.\\n%d epochs in %12.2f\" % (n_epoch, training_end_time - training_start_time))\n",
        "model.save('./data/bert_atten_SiameseLSTM.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i65LkyFsK0bE",
        "outputId": "68c8aa9c-2503-4c83-80e2-17eede25f066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " left_input (InputLayer)     [(None, 20)]                 0         []                            \n",
            "                                                                                                  \n",
            " left_input_masks (InputLay  [(None, 20)]                 0         []                            \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " right_input (InputLayer)    [(None, 20)]                 0         []                            \n",
            "                                                                                                  \n",
            " right_input_masks (InputLa  [(None, 20)]                 0         []                            \n",
            " yer)                                                                                             \n",
            "                                                                                                  \n",
            " bert_layer_3 (BertLayer)    (None, 20, 768)              1094822   ['left_input[0][0]',          \n",
            "                                                          40         'left_input_masks[0][0]',    \n",
            "                                                                     'right_input[0][0]',         \n",
            "                                                                     'right_input_masks[0][0]']   \n",
            "                                                                                                  \n",
            " self_attention_5 (SelfAtte  (None, 20, 64)               147456    ['bert_layer_3[0][0]',        \n",
            " ntion)                                                              'bert_layer_3[1][0]']        \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               (None, 20, 50)               23000     ['self_attention_5[0][0]',    \n",
            "                                                                     'self_attention_5[1][0]']    \n",
            "                                                                                                  \n",
            " global_average_pooling1d_6  (None, 50)                   0         ['lstm_1[0][0]']              \n",
            "  (GlobalAveragePooling1D)                                                                        \n",
            "                                                                                                  \n",
            " global_average_pooling1d_7  (None, 50)                   0         ['lstm_1[1][0]']              \n",
            "  (GlobalAveragePooling1D)                                                                        \n",
            "                                                                                                  \n",
            " man_dist_3 (ManDist)        (None, 1)                    0         ['global_average_pooling1d_6[0\n",
            "                                                                    ][0]',                        \n",
            "                                                                     'global_average_pooling1d_7[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109652696 (418.29 MB)\n",
            "Trainable params: 109652696 (418.29 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 328s 53s/step - loss: 0.3569 - accuracy: 0.5798 - val_loss: 0.4834 - val_accuracy: 0.3780\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 250s 49s/step - loss: 0.3678 - accuracy: 0.5401 - val_loss: 0.3435 - val_accuracy: 0.5830\n",
            "Epoch 3/10\n",
            "5/5 [==============================] - 248s 49s/step - loss: 0.4925 - accuracy: 0.4563 - val_loss: 0.6052 - val_accuracy: 0.3770\n",
            "Epoch 4/10\n",
            "5/5 [==============================] - 250s 49s/step - loss: 0.4509 - accuracy: 0.4604 - val_loss: 0.3596 - val_accuracy: 0.5950\n",
            "Epoch 5/10\n",
            "5/5 [==============================] - 250s 49s/step - loss: 0.5364 - accuracy: 0.4206 - val_loss: 0.3490 - val_accuracy: 0.4980\n",
            "Epoch 6/10\n",
            "5/5 [==============================] - 249s 49s/step - loss: 0.4159 - accuracy: 0.5247 - val_loss: 0.5908 - val_accuracy: 0.3770\n",
            "Epoch 7/10\n"
          ]
        }
      ]
    }
  ]
}